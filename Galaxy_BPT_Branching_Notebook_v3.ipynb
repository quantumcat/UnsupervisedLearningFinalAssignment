{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Galaxy Branching Workflow - Model Variations Included\n",
    "\n",
    "This notebook has two branches and four clustering families so the report and code match.\n",
    "\n",
    "**Branches**\n",
    "- A. Full-sample branch (photometry only)\n",
    "- B. Emission-line branch (BPT subset with S/N > 3 on all four lines)\n",
    "\n",
    "**Model variations on the photometry branch**\n",
    "- KMeans for k in {2,3,4,5,6}\n",
    "- Gaussian Mixture Models (GMM, covariance=\"full\") for components in {2,3,4,5,6}\n",
    "- Agglomerative clustering with linkages {\"ward\",\"complete\"} and clusters in {2,3,4,5,6}\n",
    "- DBSCAN with an eps heuristic from the k-distance curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import os, json, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.neighbors import NearestNeighbors, kneighbors_graph, KNeighborsClassifier\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (8, 6)\n",
    "plt.rcParams['axes.grid'] = False\n",
    "\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.width', 200)\n",
    "\n",
    "FIGDIR = Path('./figs')\n",
    "FIGDIR.mkdir(parents=True, exist_ok=True)\n",
    "print('Libraries imported.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Load data\n",
    "Set `DATA_PATH` to a CSV.\n",
    "The following columns should exist.\n",
    "- **Photometry / geometry:** `dered_u, dered_g, dered_r, dered_i, dered_z, petroR90_r, petroR50_r, modelMag_r`\n",
    "- **BPT lines:** `h_beta_flux, oiii_5007_flux, h_alpha_flux, nii_6584_flux` and corresponding `_err` columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_DIR = Path('out')\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# --- Deterministic loader: either ONE wide file or TWO separate files ---\n",
    "DATA_DIR = Path('.')\n",
    "GAL_PATH = DATA_DIR / 'galaxies_100k.csv'\n",
    "EM_PATH = DATA_DIR / 'emission_lines_full.csv'\n",
    "\n",
    "BPT_COLS = [\n",
    "    \"h_beta_flux\",\"oiii_5007_flux\",\"h_alpha_flux\",\"nii_6584_flux\",\n",
    "    \"h_beta_flux_err\",\"oiii_5007_flux_err\",\"h_alpha_flux_err\",\"nii_6584_flux_err\"\n",
    "]\n",
    "\n",
    "def _read_any(path):\n",
    "    if path.suffix == \".parquet\":\n",
    "        return pd.read_parquet(path)\n",
    "    if path.suffix == \".csv\":\n",
    "        return pd.read_csv(path)\n",
    "    raise ValueError(\"Use CSV or Parquet\")\n",
    "\n",
    "if EM_PATH:\n",
    "    # Two-file mode: load and merge\n",
    "    df_phot = _read_any(GAL_PATH)\n",
    "    df_spec = _read_any(EM_PATH)\n",
    "    # Choose a key present in both\n",
    "    key = \"specObjID\" if (\"specObjID\" in df_phot.columns and \"specObjID\" in df_spec.columns) else \"objID\"\n",
    "    if key not in df_phot or key not in df_spec:\n",
    "        raise RuntimeError(\"Cannot find a common key (specObjID or objID) in both files\")\n",
    "\n",
    "    # One spectrum per object: keep the one with best minimum S/N across the 4 lines\n",
    "    for line, err in [(\"h_alpha_flux\",\"h_alpha_flux_err\"),\n",
    "                      (\"h_beta_flux\",\"h_beta_flux_err\"),\n",
    "                      (\"oiii_5007_flux\",\"oiii_5007_flux_err\"),\n",
    "                      (\"nii_6584_flux\",\"nii_6584_flux_err\")]:\n",
    "        if line in df_spec and err in df_spec:\n",
    "            df_spec[f\"SN_{line}\"] = df_spec[line] / df_spec[err]\n",
    "    sn_cols = [c for c in df_spec.columns if c.startswith(\"SN_\")]\n",
    "    df_spec[\"SN_min\"] = df_spec[sn_cols].min(axis=1) if sn_cols else 0.0\n",
    "\n",
    "    spec_best = (df_spec\n",
    "                 .sort_values(\"SN_min\", ascending=False)\n",
    "                 .drop_duplicates(subset=[key], keep=\"first\"))\n",
    "\n",
    "    # Merge left to preserve the full-sample branch\n",
    "    keep_cols = [key] + [c for c in BPT_COLS if c in spec_best.columns]\n",
    "    df = df_phot.merge(spec_best[keep_cols], on=key, how=\"left\", validate=\"one_to_one\")\n",
    "else:\n",
    "    # One-file mode: wide file already\n",
    "    df = _read_any(GAL_PATH)\n",
    "\n",
    "# Diagnostics\n",
    "have_bpt = [c for c in BPT_COLS if c in df.columns]\n",
    "print(\"Rows:\", len(df), \"| BPT columns present:\", len(have_bpt), \"/\", len(BPT_COLS))\n",
    "if len(have_bpt) and len(have_bpt) < 8:\n",
    "    print(\"Partial BPT block found. You can still run the full-sample branch; BPT branch will be limited.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447ea736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load with safer inference and consistent join key\n",
    "galaxies = pd.read_csv(GAL_PATH, low_memory=False, dtype={'specObjID': str})\n",
    "emlines  = pd.read_csv(EM_PATH,  low_memory=False, dtype={'specObjID': str})\n",
    "\n",
    "galaxies['specObjID'] = galaxies['specObjID'].astype(str)\n",
    "emlines['specObjID']  = emlines['specObjID'].astype(str)\n",
    "\n",
    "# Coerce all non-key cols to numeric (invalid -> NaN) and log\n",
    "def coerce_numeric(df, key_cols=('specObjID',)):\n",
    "    coerced = []\n",
    "    for c in df.columns:\n",
    "        if c in key_cols: \n",
    "            continue\n",
    "        before_num = pd.api.types.is_numeric_dtype(df[c])\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "        if not before_num:\n",
    "            coerced.append(c)\n",
    "    return coerced\n",
    "\n",
    "coerced_g = coerce_numeric(galaxies)\n",
    "coerced_e = coerce_numeric(emlines)\n",
    "\n",
    "with open(OUT_DIR/'coercion_log.json','w') as f:\n",
    "    json.dump({'coerced_galaxies': coerced_g, 'coerced_emlines': coerced_e}, f, indent=2)\n",
    "\n",
    "df = galaxies.merge(emlines, on='specObjID', how='left')\n",
    "print('Merged shape:', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Light feature engineering (shared between both branches)\n",
    "- Colors: u-g, g-r, r-i, i-z\n",
    "- Concentration index: R90/R50\n",
    "- Surface brightness proxy: mu_r_proxy = modelMag_r + 2.5 log10(2Ï€ R50^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fe = df.copy()\n",
    "for a,b in [('u','g'), ('g','r'), ('r','i'), ('i','z')]:\n",
    "    ca, cb = f'dered_{a}', f'dered_{b}'\n",
    "    if ca in df_fe.columns and cb in df_fe.columns:\n",
    "        df_fe[f'color_{a}_minus_{b}'] = df_fe[ca] - df_fe[cb]\n",
    "if {'petroR90_r','petroR50_r'}.issubset(df_fe.columns):\n",
    "    df_fe['concentration_r'] = df_fe['petroR90_r'] / df_fe['petroR50_r']\n",
    "if {'modelMag_r','petroR50_r'}.issubset(df_fe.columns):\n",
    "    r50 = df_fe['petroR50_r'].replace({0: np.nan})\n",
    "    df_fe['mu_r_proxy'] = df_fe['modelMag_r'] + 2.5*np.log10(2*np.pi*(r50**2))\n",
    "print('Feature engineering complete. Shape:', df_fe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BPT columns and missingness pattern\n",
    "We treat the eight BPT columns as non-imputable and use them only in the emission-line branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "present_bpt = [c for c in BPT_COLS if c in df_fe.columns]\n",
    "missing_block_mask = df_fe[present_bpt].isna().all(axis=1) if present_bpt else pd.Series(False, index=df_fe.index)\n",
    "print('BPT columns present:', present_bpt)\n",
    "print('Rows missing all present BPT columns:', int(missing_block_mask.sum()), '/', len(df_fe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_matrix(df_in, include_cols, exclude_cols=None, impute=True, scale=True):\n",
    "    exclude_cols = set(exclude_cols or [])\n",
    "    cols = [c for c in include_cols if c in df_in.columns]\n",
    "    impute_cols = [c for c in cols if c not in exclude_cols]\n",
    "    X_parts, used_cols = [], []\n",
    "    if impute_cols:\n",
    "        X_imp = df_in[impute_cols].to_numpy(dtype=np.float32)\n",
    "        if impute:\n",
    "            X_imp = SimpleImputer(strategy='median').fit_transform(X_imp).astype(np.float32)\n",
    "        # Drop near-constant features to avoid zero-IQR scaling\n",
    "        q75 = np.nanpercentile(X_imp, 75, axis=0)\n",
    "        q25 = np.nanpercentile(X_imp, 25, axis=0)\n",
    "        iqr = q75 - q25\n",
    "        keep_idx = np.where(iqr > 1e-6)[0]\n",
    "        if len(keep_idx) < X_imp.shape[1]:\n",
    "            print('Dropping near-constant features:', int(X_imp.shape[1]-len(keep_idx)))\n",
    "        X_imp = X_imp[:, keep_idx]\n",
    "        used_cols = [impute_cols[i] for i in keep_idx]\n",
    "        if scale:\n",
    "            X_imp = RobustScaler().fit_transform(X_imp).astype(np.float32)\n",
    "        X_parts.append(X_imp)\n",
    "    X = np.concatenate(X_parts, axis=1) if X_parts else np.empty((len(df_in), 0), dtype=np.float32)\n",
    "    return X, used_cols\n",
    "\n",
    "def kewley_2001(x):\n",
    "    return 0.61/(x - 0.47) + 1.19\n",
    "def kauffmann_2003(x):\n",
    "    return 0.61/(x - 0.05) + 1.30\n",
    "def plot_bpt_annotated(df_bpt, title='BPT (S/N>3 on all lines)'):\n",
    "    x = np.log10(df_bpt['nii_6584_flux'] / df_bpt['h_alpha_flux'])\n",
    "    y = np.log10(df_bpt['oiii_5007_flux'] / df_bpt['h_beta_flux'])\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    hb = ax.hexbin(x, y, gridsize=70, mincnt=1)\n",
    "    fig.colorbar(hb, ax=ax, label='Counts/bin')\n",
    "    xx = np.linspace(-1.6, 0.6, 1000)\n",
    "    mask_kew = xx < (0.47 - 1e-6)\n",
    "    mask_kau = xx < (0.05 - 1e-6)\n",
    "    ax.plot(xx[mask_kew], kewley_2001(xx[mask_kew]), '--', label='Kewley 2001')\n",
    "    ax.plot(xx[mask_kau], kauffmann_2003(xx[mask_kau]), ':',  label='Kauffmann 2003')\n",
    "    ax.set_xlim(-1.6, 0.6); ax.set_ylim(-1.2, 1.5)\n",
    "    ax.set_xlabel('log10([NII] 6584 / Halpha)')\n",
    "    ax.set_ylabel('log10([OIII] 5007 / Hbeta)')\n",
    "    ax.set_title(title); ax.legend(loc='lower right')\n",
    "    ax.text(-1.25, -0.75, 'Star-forming', fontsize=10, color='white',\n",
    "            bbox=dict(facecolor='black', alpha=0.5, boxstyle='round,pad=0.2'))\n",
    "    ax.text(-0.55,  0.45, 'Composite',    fontsize=10, color='white',\n",
    "            bbox=dict(facecolor='black', alpha=0.5, boxstyle='round,pad=0.2'))\n",
    "    ax.text( 0.10,  0.85, 'AGN',          fontsize=10, color='white',\n",
    "            bbox=dict(facecolor='black', alpha=0.5, boxstyle='round,pad=0.2'))\n",
    "    plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# A. Full-sample branch - Photometry only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_like = {'specObjID','objID','flags','type','mode'}\n",
    "photometric_like = [c for c in df_fe.columns if c.startswith('dered_') or c in {\n",
    "    'modelMag_r','petroR90_r','petroR50_r','concentration_r','mu_r_proxy'\n",
    "} or c.startswith('color_')]\n",
    "full_cols = [c for c in photometric_like if c not in id_like and pd.api.types.is_numeric_dtype(df_fe[c])]\n",
    "X_full, cols_full = build_feature_matrix(df_fe, include_cols=full_cols, exclude_cols=None, impute=True, scale=True)\n",
    "print('Full branch matrix:', X_full.shape, '| features:', len(cols_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_full = PCA(n_components=10, random_state=0).fit(X_full)\n",
    "plt.plot(np.cumsum(pca_full.explained_variance_ratio_), marker='o')\n",
    "plt.xlabel('Components'); plt.ylabel('Cumulative explained variance')\n",
    "plt.title('Full-sample PCA variance curve')\n",
    "plt.tight_layout(); plt.savefig(FIGDIR/'fig_pca_variance.png', dpi=175); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3062a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Inspect what went into PCA\n",
    "print(\"X_full shape:\", X_full.shape)\n",
    "print(\"Any inf/NaN? \", ~np.isfinite(X_full).all())\n",
    "\n",
    "# 2) Find blown-up rows\n",
    "row_norm = np.linalg.norm(X_full, axis=1)\n",
    "print(\"Top 5 row norms:\", np.sort(row_norm)[-5:])\n",
    "bad_rows = np.where(row_norm > np.percentile(row_norm, 99.9))[0]\n",
    "print(\"Potential outlier rows:\", bad_rows[:10])\n",
    "\n",
    "# 3) Check which features cause it (near-zero IQR after imputation)\n",
    "from sklearn.impute import SimpleImputer\n",
    "X_imp = SimpleImputer(strategy='median').fit_transform(df_fe[full_cols].to_numpy(dtype=np.float64))\n",
    "q75 = np.nanpercentile(X_imp, 75, axis=0)\n",
    "q25 = np.nanpercentile(X_imp, 25, axis=0)\n",
    "iqr = q75 - q25\n",
    "small_iqr_idx = np.where(iqr < 1e-6)[0]\n",
    "small_iqr_cols = [full_cols[i] for i in small_iqr_idx]\n",
    "print(\"Near-constant (tiny IQR) features:\", small_iqr_cols[:20], f\"(total {len(small_iqr_cols)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop near-constant features before scaling\n",
    "keep = [c for i,c in enumerate(full_cols) if i not in set(small_iqr_idx)]\n",
    "X_imp = SimpleImputer(strategy='median').fit_transform(df_fe[keep].to_numpy(dtype=np.float32))\n",
    "X_full = RobustScaler().fit_transform(X_imp).astype(np.float32)\n",
    "\n",
    "# Winsorize extreme rows to be safe\n",
    "row_norm = np.linalg.norm(X_full, axis=1)\n",
    "cap = np.percentile(row_norm, 99.9)\n",
    "X_full[row_norm > cap] *= (cap / row_norm[row_norm > cap])[:, None]\n",
    "\n",
    "km_full = KMeans(n_clusters=3, n_init=10, random_state=0)\n",
    "labels_full_k3 = km_full.fit_predict(X_full)\n",
    "Z2 = PCA(n_components=2, random_state=0).fit_transform(X_full)\n",
    "plt.scatter(Z2[:,0], Z2[:,1], s=6, c=labels_full_k3, alpha=0.65)\n",
    "plt.title('Full-sample: PCA(2) + KMeans(k=3)')\n",
    "plt.xlabel('PC1'); plt.ylabel('PC2')\n",
    "plt.tight_layout(); plt.savefig(FIGDIR/'fig_pca_kmeans.png', dpi=175); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# C. Model variations on the photometry branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a03e53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep_gmm_robust(X, comps=(2,3,4,5,6),\n",
    "                     cov_types=('full','diag'),\n",
    "                     reg_list=(1e-6, 1e-5, 1e-4),\n",
    "                     seed=0, max_iter=500, n_init=3):\n",
    "    X = np.asarray(X, dtype=np.float64)  # <- critical\n",
    "    rows = []\n",
    "    for cov in cov_types:\n",
    "        for k in comps:\n",
    "            fitted = False\n",
    "            for reg in reg_list:\n",
    "                try:\n",
    "                    gmm = GaussianMixture(\n",
    "                        n_components=k,\n",
    "                        covariance_type=cov,\n",
    "                        reg_covar=reg,\n",
    "                        init_params='kmeans',\n",
    "                        n_init=n_init,\n",
    "                        random_state=seed,\n",
    "                        max_iter=max_iter,\n",
    "                        tol=1e-3\n",
    "                    )\n",
    "                    labels = gmm.fit_predict(X)\n",
    "                    sil = silhouette_score(X, labels) if len(set(labels)) > 1 else np.nan\n",
    "                    rows.append({\n",
    "                        \"model\": \"GMM\",\n",
    "                        \"covariance\": cov,\n",
    "                        \"k\": k,\n",
    "                        \"reg_covar\": reg,\n",
    "                        \"silhouette\": sil,\n",
    "                        \"BIC\": gmm.bic(X),\n",
    "                        \"AIC\": gmm.aic(X),\n",
    "                        \"status\": \"ok\"\n",
    "                    })\n",
    "                    fitted = True\n",
    "                    break  # stop escalating reg once it worked\n",
    "                except ValueError as e:\n",
    "                    rows.append({\n",
    "                        \"model\": \"GMM\",\n",
    "                        \"covariance\": cov,\n",
    "                        \"k\": k,\n",
    "                        \"reg_covar\": reg,\n",
    "                        \"silhouette\": np.nan,\n",
    "                        \"BIC\": np.nan,\n",
    "                        \"AIC\": np.nan,\n",
    "                        \"status\": f\"fail: {str(e)[:80]}\"\n",
    "                    })\n",
    "            if not fitted:\n",
    "                print(f\"[GMM] all reg_covar attempts failed for k={k}, cov={cov}\")\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f15a904",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep_kmeans(X, k_list=(2,3,4,5,6), seed=0):\n",
    "    rows=[]\n",
    "    for k in k_list:\n",
    "        print(\"Running kmeans for \", k)\n",
    "        km = KMeans(n_clusters=k, n_init=10, random_state=seed)\n",
    "        lab = km.fit_predict(X)\n",
    "        sil = silhouette_score(X, lab) if len(set(lab))>1 else np.nan\n",
    "        rows.append({'model':'KMeans','k':k,'silhouette':sil})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def sweep_gmm(X, comps=(2,3,4,5,6), cov='full', seed=0):\n",
    "    rows=[]\n",
    "    for k in comps:\n",
    "        gmm = GaussianMixture(n_components=k, covariance_type=cov, random_state=seed)\n",
    "        lab = gmm.fit_predict(X)\n",
    "        sil = silhouette_score(X, lab) if len(set(lab))>1 else np.nan\n",
    "        rows.append({'model':f'GMM-{cov}','k':k,'silhouette':sil,'BIC':gmm.bic(X),'AIC':gmm.aic(X)})\n",
    "    return pd.DataFrame(rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea680582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep_agglom_knn(X, k_list=(2,3,4,5,6), linkage='complete',\n",
    "                     n_neighbors=25, seed=0, max_points=60000):\n",
    "    \"\"\"\n",
    "    Agglomerative with sparse kNN connectivity.\n",
    "    If X is huge, subsample to max_points, then optionally propagate labels.\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, dtype=np.float32)\n",
    "    n = X.shape[0]\n",
    "\n",
    "    subsample_idx = None\n",
    "    X_work = X\n",
    "    if n > max_points:\n",
    "        rng = np.random.default_rng(seed)\n",
    "        subsample_idx = rng.choice(n, size=max_points, replace=False)\n",
    "        X_work = X[subsample_idx]\n",
    "\n",
    "    conn = kneighbors_graph(X_work, n_neighbors=n_neighbors,\n",
    "                            mode='distance', include_self=False, n_jobs=-1)\n",
    "\n",
    "    rows, labels_out = [], {}\n",
    "    for k in k_list:\n",
    "        agg = AgglomerativeClustering(n_clusters=k, linkage=linkage, connectivity=conn)\n",
    "        lab_sub = agg.fit_predict(X_work)\n",
    "        sil = silhouette_score(X_work, lab_sub) if len(set(lab_sub)) > 1 else np.nan\n",
    "        rows.append({\"model\": f\"Agglomerative-{linkage}\",\n",
    "                     \"k\": k, \"silhouette\": sil,\n",
    "                     \"n_used\": X_work.shape[0],\n",
    "                     \"neighbors\": n_neighbors,\n",
    "                     \"subsample\": subsample_idx is not None})\n",
    "        labels_out[k] = (lab_sub, subsample_idx)\n",
    "    return pd.DataFrame(rows), labels_out\n",
    "\n",
    "def propagate_labels_knn(X_all, label_sub, subsample_idx, n_neighbors=7):\n",
    "    knn = KNeighborsClassifier(n_neighbors=n_neighbors, weights='distance')\n",
    "    knn.fit(X_all[subsample_idx], label_sub)\n",
    "    return knn.predict(X_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running kmeans\")\n",
    "km_df  = sweep_kmeans(X_full)\n",
    "\n",
    "print(\"Running gmm\")\n",
    "gmm_df = sweep_gmm_robust(X_full, cov_types=('full','diag'))\n",
    "\n",
    "print(\"Running Agglomerative-complete with kNN connectivityâ€¦\")\n",
    "agg_c_df, agg_labels = sweep_agglom_knn(X_full, linkage='complete', n_neighbors=25, max_points=60000)\n",
    "display(agg_c_df.sort_values('silhouette', ascending=False).round(3))\n",
    "\n",
    "best = agg_c_df.sort_values('silhouette', ascending=False).iloc[0]\n",
    "k_best_c = int(best['k'])\n",
    "lab_sub, subs_idx = agg_labels[k_best_c]\n",
    "labels_full_agg_c = (propagate_labels_knn(X_full, lab_sub, subs_idx, n_neighbors=7)\n",
    "                     if subs_idx is not None else lab_sub)\n",
    "\n",
    "print(\"Summarizing\")\n",
    "summary = pd.concat([km_df, gmm_df, agg_c_df], ignore_index=True)\n",
    "display(summary.sort_values(['model','k']).round(3))\n",
    "summary.to_csv(FIGDIR/'model_sweep_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z2_full = PCA(n_components=2, random_state=0).fit_transform(X_full)\n",
    "\n",
    "def best_row(df, prefix):\n",
    "    dfm = df[df[\"model\"].str.startswith(prefix)]\n",
    "    return None if dfm.empty else dfm.sort_values(\"silhouette\", ascending=False).head(1).iloc[0]\n",
    "\n",
    "# KMeans\n",
    "row = best_row(summary, \"KMeans\")\n",
    "if row is not None:\n",
    "    k = int(row[\"k\"])\n",
    "    lab = KMeans(n_clusters=k, n_init=10, random_state=0).fit_predict(X_full)\n",
    "    plt.scatter(Z2_full[:,0], Z2_full[:,1], s=6, c=lab, alpha=0.65)\n",
    "    plt.title(f\"KMeans best (k={k})\"); plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
    "    plt.tight_layout(); plt.savefig(FIGDIR/'fig_kmeans_best.png', dpi=175); plt.show()\n",
    "\n",
    "# GMM\n",
    "row = best_row(summary, \"GMM\")\n",
    "if row is not None and pd.notna(row[\"silhouette\"]):\n",
    "    k = int(row[\"k\"]); cov = row[\"covariance\"]; reg = float(row[\"reg_covar\"])\n",
    "    gmm = GaussianMixture(n_components=k, covariance_type=cov, reg_covar=reg,\n",
    "                          init_params='kmeans', n_init=3, random_state=0, max_iter=500)\n",
    "    lab = gmm.fit_predict(np.asarray(X_full, dtype=np.float64))\n",
    "    plt.scatter(Z2_full[:,0], Z2_full[:,1], s=6, c=lab, alpha=0.65)\n",
    "    plt.title(f\"GMM best (k={k}, cov={cov}, reg={reg:g})\"); plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
    "    plt.tight_layout(); plt.savefig(FIGDIR/'fig_gmm_best.png', dpi=175); plt.show()\n",
    "\n",
    "# Agglomerative-complete (use the labels we just built)\n",
    "plt.scatter(Z2_full[:,0], Z2_full[:,1], s=6, c=labels_full_agg_c, alpha=0.65)\n",
    "plt.title(f\"Agglomerative-complete best (k={k_best_c})\"); plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
    "plt.tight_layout(); plt.savefig(FIGDIR/'fig_agglomerative_complete_best.png', dpi=175); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "nbrs = NearestNeighbors(n_neighbors=k).fit(X_full)\n",
    "distances, _ = nbrs.kneighbors(X_full)\n",
    "kdist = np.sort(distances[:,k-1])\n",
    "plt.plot(kdist)\n",
    "plt.title('DBSCAN k-distance curve (k=10)')\n",
    "plt.xlabel('Points sorted by distance to k-th neighbor')\n",
    "plt.ylabel('Distance')\n",
    "plt.tight_layout(); plt.savefig(FIGDIR/'fig_dbscan_kdist.png', dpi=175); plt.show()\n",
    "\n",
    "eps = float(np.percentile(kdist, 95))\n",
    "db = DBSCAN(eps=eps, min_samples=k).fit(X_full)\n",
    "labels_db = db.labels_\n",
    "n_noise = int(np.sum(labels_db == -1))\n",
    "n_clusters_db = len(set(labels_db)) - (1 if -1 in labels_db else 0)\n",
    "print(f'DBSCAN -> eps={eps:.3f}, min_samples={k}, clusters={n_clusters_db}, noise={n_noise}')\n",
    "\n",
    "mask_core = labels_db != -1\n",
    "sil_db = np.nan\n",
    "if len(set(labels_db[mask_core])) > 1:\n",
    "    sil_db = silhouette_score(X_full[mask_core], labels_db[mask_core])\n",
    "print('DBSCAN silhouette (core only):', None if np.isnan(sil_db) else round(sil_db, 3))\n",
    "\n",
    "Z2 = PCA(n_components=2, random_state=0).fit_transform(X_full)\n",
    "plt.scatter(Z2[mask_core,0], Z2[mask_core,1], s=6, c=labels_db[mask_core], alpha=0.65)\n",
    "plt.title('DBSCAN clusters (core only) in PCA(2)')\n",
    "plt.xlabel('PC1'); plt.ylabel('PC2')\n",
    "plt.tight_layout(); plt.savefig(FIGDIR/'fig_dbscan_core.png', dpi=175); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# B. Emission-line branch - BPT subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required = ['h_beta_flux','oiii_5007_flux','h_alpha_flux','nii_6584_flux',\n",
    "            'h_beta_flux_err','oiii_5007_flux_err','h_alpha_flux_err','nii_6584_flux_err']\n",
    "present = [c for c in required if c in df_fe.columns]\n",
    "if len(present) < 8:\n",
    "    print('Not all BPT columns are present - skipping BPT branch in this run.')\n",
    "else:\n",
    "    pos_mask = (df_fe['h_beta_flux']>0) & (df_fe['oiii_5007_flux']>0) & (df_fe['h_alpha_flux']>0) & (df_fe['nii_6584_flux']>0)\n",
    "    sn_mask  = (df_fe['h_beta_flux']/df_fe['h_beta_flux_err']>3) & (df_fe['oiii_5007_flux']/df_fe['oiii_5007_flux_err']>3) & \\\n",
    "               (df_fe['h_alpha_flux']/df_fe['h_alpha_flux_err']>3) & (df_fe['nii_6584_flux']/df_fe['nii_6584_flux_err']>3)\n",
    "    em_mask = pos_mask & sn_mask\n",
    "    df_em = df_fe.loc[em_mask].copy()\n",
    "    print('Emission-line subset size:', df_em.shape)\n",
    "    # BPT with hexbin background\n",
    "    x = np.log10(df_em['nii_6584_flux'] / df_em['h_alpha_flux'])\n",
    "    y = np.log10(df_em['oiii_5007_flux'] / df_em['h_beta_flux'])\n",
    "    fig, ax = plt.subplots(figsize=(7.5,5.5))\n",
    "    hb = ax.hexbin(x, y, gridsize=70, bins='log', mincnt=1)\n",
    "    xx = np.linspace(-1.6, 0.6, 1000)\n",
    "    ax.plot(xx[xx<0.47-1e-6], kewley_2001(xx[xx<0.47-1e-6]), '--', label='Kewley 2001')\n",
    "    ax.plot(xx[xx<0.05-1e-6], kauffmann_2003(xx[xx<0.05-1e-6]), ':',  label='Kauffmann 2003')\n",
    "    ax.set_xlim(-1.6, 0.6); ax.set_ylim(-1.2, 1.5)\n",
    "    ax.set_xlabel('log10([NII] 6584 / Halpha)'); ax.set_ylabel('log10([OIII] 5007 / Hbeta)')\n",
    "    ax.set_title('BPT hexbin density')\n",
    "    plt.tight_layout(); plt.savefig(FIGDIR/'fig_bpt_density.png', dpi=175); plt.show()\n",
    "\n",
    "    # Map photometric clusters onto emission-line subset and color BPT by cluster\n",
    "    labels_series = pd.Series(labels_full_k3, index=df_fe.index, name='cluster')\n",
    "    df_em2 = df_em.join(labels_series, how='left')\n",
    "    n2ha = np.log10(df_em2['nii_6584_flux']/df_em2['h_alpha_flux'])\n",
    "    o3hb = np.log10(df_em2['oiii_5007_flux']/df_em2['h_beta_flux'])\n",
    "    plt.figure(figsize=(7.5,5.5))\n",
    "    plt.scatter(n2ha, o3hb, s=6, c=df_em2['cluster'], alpha=0.6)\n",
    "    xx = np.linspace(-1.6, 0.6, 1000)\n",
    "    plt.plot(xx[xx<0.47-1e-6], kewley_2001(xx[xx<0.47-1e-6]), '--')\n",
    "    plt.plot(xx[xx<0.05-1e-6], kauffmann_2003(xx[xx<0.05-1e-6]), ':')\n",
    "    plt.xlim(-1.6, 0.6); plt.ylim(-1.2, 1.5)\n",
    "    plt.xlabel('log10([NII] 6584 / Halpha)'); plt.ylabel('log10([OIII] 5007 / Hbeta)')\n",
    "    plt.title('BPT colored by photometric clusters')\n",
    "    plt.tight_layout(); plt.savefig(FIGDIR/'fig_bpt_by_cluster.png', dpi=175); plt.show()\n",
    "\n",
    "    # BPT class, crosstab, and chi-square\n",
    "    def bpt_class(n2, o3):\n",
    "        if o3 > kewley_2001(n2):\n",
    "            return 'AGN'\n",
    "        if o3 > kauffmann_2003(n2):\n",
    "            return 'Composite'\n",
    "        return 'Star-forming'\n",
    "    df_em2 = df_em2.assign(\n",
    "        n2ha = np.log10(df_em2['nii_6584_flux']/df_em2['h_alpha_flux']),\n",
    "        o3hb = np.log10(df_em2['oiii_5007_flux']/df_em2['h_beta_flux'])\n",
    "    )\n",
    "    df_em2['BPT'] = [bpt_class(n, o) for n,o in zip(df_em2['n2ha'], df_em2['o3hb'])]\n",
    "    xtab = pd.crosstab(df_em2['cluster'], df_em2['BPT'])\n",
    "    print('Crosstab counts:\\n', xtab)\n",
    "    chi2, p, dof, _ = chi2_contingency(xtab)\n",
    "    print(f'Chi-square={chi2:.1f}, dof={dof}, p={p:.2e}')\n",
    "    xtab.to_csv(FIGDIR/'bpt_crosstab_counts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410c5a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
